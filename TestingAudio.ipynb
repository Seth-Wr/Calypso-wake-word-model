{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5731bfc-94fc-4a55-9b6c-bf60f4ef86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import deque\n",
    "import datetime\n",
    "from playsound3 import playsound\n",
    "import threading\n",
    "import simpleaudio as sa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b621967-1178-4f0e-92e4-d7c75d9298b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "CHUNK = 1024  # Buffer size\n",
    "FORMAT = pyaudio.paInt16  # 16-bit resolution\n",
    "CHANNELS = 1  # Mono audio\n",
    "RATE = 16000  # sampling rate\n",
    "\n",
    "def play_sound(path):\n",
    "    try:\n",
    "        wave_obj = sa.WaveObject.from_wave_file(path)\n",
    "        wave_obj.play()  # non-blocking\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Could not play sound:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d9ac16b-29bb-4788-a8f6-c892dcef918e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_wav_16k_mono_from_buffer(audio_bytes, dtype=np.int16):\n",
    "    \"\"\"\n",
    "    Converts raw audio bytes from PyAudio buffer to float32 tensor\n",
    "    equivalent to tf.audio.decode_wav(..., desired_channels=1)\n",
    "    \"\"\"\n",
    "    # Convert bytes to int16 NumPy array\n",
    "    audio_np = np.frombuffer(audio_bytes, dtype=dtype)\n",
    "\n",
    "    # Normalize to float32 in [-1.0, 1.0]\n",
    "    audio_float32 = audio_np.astype(np.float32) / 32768.0\n",
    "\n",
    "    # Convert to TensorFlow tensor\n",
    "    audio_tensor = tf.convert_to_tensor(audio_float32, dtype=tf.float32)\n",
    "\n",
    "    return audio_tensor  # shape: [samples], dtype: float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ceb57aa-7142-48ac-9cc3-b480628cd8be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_SECONDS = 1.25             # ≈1.25 s\n",
    "NUM_SAMPLES = int(SAMPLE_RATE * DURATION_SECONDS)  # 20000\n",
    "NUM_MELS = 40\n",
    "FRAME_LENGTH = 512\n",
    "FRAME_STEP = 160                    # 10 ms step\n",
    "FFT_LENGTH = 512\n",
    "FMIN = 80.0\n",
    "FMAX = 7600.0\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 30\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e6c5e6e-7fb1-4fe6-8f32-1fcadacff2c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_from_buffer(audio_tensor):\n",
    "    \"\"\"\n",
    "    Convert a raw audio buffer to a normalized log-mel spectrogram\n",
    "    compatible with the training pipeline.\n",
    "    \"\"\"\n",
    "    # Ensure correct length (pad or trim to NUM_SAMPLES = 20000)\n",
    "    audio_tensor = audio_tensor[:NUM_SAMPLES]\n",
    "    pad_len = NUM_SAMPLES - tf.shape(audio_tensor)[0]\n",
    "    audio_tensor = tf.pad(audio_tensor, [[0, pad_len]])\n",
    "\n",
    "    # Pre-emphasis filter to boost high frequencies\n",
    "    emphasized = tf.concat([audio_tensor[:1], audio_tensor[1:] - 0.97 * audio_tensor[:-1]], axis=0)\n",
    "\n",
    "    # Compute STFT\n",
    "    stft = tf.signal.stft(\n",
    "        emphasized,\n",
    "        frame_length=FRAME_LENGTH,\n",
    "        frame_step=FRAME_STEP,\n",
    "        fft_length=FFT_LENGTH,\n",
    "        window_fn=tf.signal.hann_window\n",
    "    )\n",
    "    mag = tf.abs(stft)\n",
    "\n",
    "    # Apply mel filterbank\n",
    "    mel_weight = tf.signal.linear_to_mel_weight_matrix(\n",
    "        NUM_MELS,\n",
    "        mag.shape[-1],\n",
    "        SAMPLE_RATE,\n",
    "        FMIN,\n",
    "        FMAX\n",
    "    )\n",
    "    mel = tf.matmul(mag, mel_weight)\n",
    "\n",
    "    # Log scale and normalization\n",
    "    log_mel = tf.math.log(mel + 1e-6)\n",
    "    mean = tf.reduce_mean(log_mel)\n",
    "    std = tf.math.reduce_std(log_mel) + 1e-6\n",
    "    log_mel = (log_mel - mean) / std\n",
    "\n",
    "    # Add channel dimension\n",
    "    log_mel = tf.expand_dims(log_mel, axis=-1)\n",
    "\n",
    "    return log_mel  # shape: [time_frames, NUM_MELS, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d75a40d-7ab2-4168-91d0-b4518a5b42a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input details: [{'name': 'serving_default_input_4:0', 'index': 0, 'shape': array([  1, 122,  40,   1], dtype=int32), 'shape_signature': array([ -1, 122,  40,   1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output details: [{'name': 'StatefulPartitionedCall:0', 'index': 33, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "model_path=\"/Users/sethwright/Documents/audio-model/output/Sheila_float32.tflite\"\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path)\n",
    "interpreter.allocate_tensors()\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Print input/output details\n",
    "print(\"Input details:\", input_details)\n",
    "print(\"Output details:\", output_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3058679c-65e4-4661-a005-95eb72bb9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#run real time predictions with keras model\n",
    "def test_audio_tflite():\n",
    "    # Initialize PyAudio\n",
    "    audio = pyaudio.PyAudio() \n",
    "    audio_buffer = deque(maxlen=20000)\n",
    "\n",
    "\n",
    "    # Sliding window for smoothing predictions\n",
    "    OUT_DIR = (\"/Users/sethwright/Documents/audio-model/data\")\n",
    "    window = deque(maxlen=3)\n",
    "    THRESHOLD = 0.7  # Adjust this to be stricter\n",
    "    FALSE_POS_DIR = os.path.join(OUT_DIR, \"false_positives\")\n",
    "    os.makedirs(FALSE_POS_DIR, exist_ok=True)\n",
    "    \n",
    "    # List input devices\n",
    "    #print(\"Available input devices:\")\n",
    "    for i in range(audio.get_device_count()):\n",
    "        info = audio.get_device_info_by_index(i)\n",
    "        #if info[\"maxInputChannels\"] > 0:\n",
    "        #print(f\"  [{i}] {info['name']}\")\n",
    "    # Open stream\n",
    "    stream = audio.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "    print(\"listening started\")\n",
    "    \n",
    "    while True:\n",
    "        data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        samples = np.frombuffer(data, dtype=np.int16).astype(np.float32)\n",
    "        audio_buffer.extend(samples)\n",
    "    \n",
    "        if len(audio_buffer) == NUM_SAMPLES:\n",
    "            wav_tensor = np.array(audio_buffer, dtype=np.float32)\n",
    "            spectrogram = preprocess_from_buffer(wav_tensor)\n",
    "            spectrogram = tf.expand_dims(spectrogram, axis=0)\n",
    "\n",
    "            interpreter.set_tensor(input_details[0]['index'], spectrogram)\n",
    "            interpreter.invoke()\n",
    "            pred = interpreter.get_tensor(output_details[0]['index'])[0][0]\n",
    "\n",
    "            # Update sliding window\n",
    "            window.append(pred)\n",
    "    \n",
    "            # Trigger only if all predictions in window exceed threshold\n",
    "            if len(window) == 3 and all(p > THRESHOLD for p in window):\n",
    "                print(\"Wake word detected!\")\n",
    "                print(f\"Model raw output: {pred:.4f}\")\n",
    "                #playsound(\"/Users/sethwright/Downloads/gong.mp3\")\n",
    "                play_sound(\"/Users/sethwright/Downloads/gong.wav\")\n",
    "\n",
    "                # Save detected audio\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = os.path.join(OUT_DIR, f\"Positive_{timestamp}_{pred:.4f}.wav\")\n",
    "                with wave.open(filename, 'wb') as wf:\n",
    "                    wf.setnchannels(CHANNELS)\n",
    "                    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "                    wf.setframerate(RATE)\n",
    "                    wf.writeframes(np.array(audio_buffer, dtype=np.int16).tobytes())\n",
    "                print(f\"Audio saved: {filename}\")\n",
    "    \n",
    "                # Clear buffer for next detection\n",
    "                audio_buffer.clear()\n",
    "                window.clear()\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984dc98-bc62-4670-b9d7-fa2ad1ce2962",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "# audio_buffer = deque(28000)\n",
    "\n",
    "# List input devices\n",
    "print(\"Available input devices:\")\n",
    "for i in range(audio.get_device_count()):\n",
    "    info = audio.get_device_info_by_index(i)\n",
    "    if info[\"maxInputChannels\"] > 0:\n",
    "        print(f\"  [{i}] {info['name']}\")\n",
    "# Open stream\n",
    "stream = audio.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"\\nRecording...\")\n",
    "try:\n",
    "    while True:\n",
    "        audio_buffer = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        # Now process it\n",
    "        wav_tensor = load_wav_16k_mono_from_buffer(audio_buffer)\n",
    "        spectrogram = preprocess_from_buffer(wav_tensor)\n",
    "        spectrogram = tf.expand_dims(spectrogram, axis=0)\n",
    "\n",
    "        print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "        interpreter.set_tensor(input_details[0]['index'], spectrogram)\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "        # Get the output\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output_data_number = [1 if output_data > .5 else 0] \n",
    "        print(\"Output:\", output_data, output_data_number)\n",
    "        # You can now run inference here...\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by user.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7032f35-9191-4651-9fa0-0656d1da8828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded: /Users/sethwright/Documents/audio-model/output/saved_model.keras\n"
     ]
    }
   ],
   "source": [
    "# Load trained Keras model\n",
    "# -----------------------\n",
    "MODEL_PATH=\"/Users/sethwright/Documents/audio-model/output/saved_model.keras\"\n",
    "model = tf.keras.models.load_model(MODEL_PATH, compile =False, safe_mode=False)\n",
    "print(\"✅ Model loaded:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2238154-24f7-40d4-84f5-8460a0190a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run real time predictions with keras model\n",
    "def test_audio():\n",
    "    # Initialize PyAudio\n",
    "    audio = pyaudio.PyAudio() \n",
    "    audio_buffer = deque(maxlen=20000)\n",
    "\n",
    "\n",
    "    # Sliding window for smoothing predictions\n",
    "    OUT_DIR = (\"/Users/sethwright/Documents/audio-model/data\")\n",
    "    window = deque(maxlen=3)\n",
    "    THRESHOLD = 0.7  # Adjust this to be stricter\n",
    "    FALSE_POS_DIR = os.path.join(OUT_DIR, \"false_positives\")\n",
    "    os.makedirs(FALSE_POS_DIR, exist_ok=True)\n",
    "    \n",
    "    # List input devices\n",
    "    #print(\"Available input devices:\")\n",
    "    for i in range(audio.get_device_count()):\n",
    "        info = audio.get_device_info_by_index(i)\n",
    "        #if info[\"maxInputChannels\"] > 0:\n",
    "        #print(f\"  [{i}] {info['name']}\")\n",
    "    # Open stream\n",
    "    stream = audio.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "    print(\"listening started\")\n",
    "    \n",
    "    while True:\n",
    "        data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        samples = np.frombuffer(data, dtype=np.int16).astype(np.float32)\n",
    "        audio_buffer.extend(samples)\n",
    "    \n",
    "        if len(audio_buffer) == NUM_SAMPLES:\n",
    "            wav_tensor = np.array(audio_buffer, dtype=np.float32)\n",
    "            spectrogram = preprocess_from_buffer(wav_tensor)\n",
    "            spectrogram = tf.expand_dims(spectrogram, axis=0)\n",
    "    \n",
    "            pred = model.predict(spectrogram, verbose=0)[0][0]\n",
    "    \n",
    "            # Update sliding window\n",
    "            window.append(pred)\n",
    "    \n",
    "            # Trigger only if all predictions in window exceed threshold\n",
    "            if len(window) == 3 and all(p > THRESHOLD for p in window):\n",
    "                print(\"Wake word detected!\")\n",
    "                print(f\"Model raw output: {pred:.4f}\")\n",
    "                #playsound(\"/Users/sethwright/Downloads/gong.mp3\")\n",
    "                play_sound(\"/Users/sethwright/Downloads/gong.wav\")\n",
    "\n",
    "                # Save detected audio\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = os.path.join(OUT_DIR, f\"Positive_{timestamp}_{pred:.4f}.wav\")\n",
    "                with wave.open(filename, 'wb') as wf:\n",
    "                    wf.setnchannels(CHANNELS)\n",
    "                    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "                    wf.setframerate(RATE)\n",
    "                    wf.writeframes(np.array(audio_buffer, dtype=np.int16).tobytes())\n",
    "                print(f\"Audio saved: {filename}\")\n",
    "    \n",
    "                # Clear buffer for next detection\n",
    "                audio_buffer.clear()\n",
    "                window.clear()\n",
    "                \n",
    "    '''\n",
    "            # Save false positives automatically\n",
    "            elif len(window) == 3 and all(p > THRESHOLD for p in window) == False and pred > 0.5:\n",
    "                # Example: false positive detected\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = os.path.join(FALSE_POS_DIR, f\"FalsePositive_{timestamp}_{pred:.4f}.wav\")\n",
    "                with wave.open(filename, 'wb') as wf:\n",
    "                    wf.setnchannels(CHANNELS)\n",
    "                    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "                    wf.setframerate(RATE)\n",
    "                    wf.writeframes(np.array(audio_buffer, dtype=np.int16).tobytes())\n",
    "                print(f\"False positive saved: {filename}\")\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d2188a-cdd6-4709-94c1-165628e0e7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listening started\n"
     ]
    }
   ],
   "source": [
    "test_audio_tflite()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-kernel",
   "language": "python",
   "name": "audio-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

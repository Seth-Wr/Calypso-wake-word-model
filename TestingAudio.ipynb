{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5731bfc-94fc-4a55-9b6c-bf60f4ef86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from playsound3 import playsound\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b621967-1178-4f0e-92e4-d7c75d9298b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "CHUNK = 400  # Buffer size\n",
    "FORMAT = pyaudio.paInt16  # 16-bit resolution\n",
    "CHANNELS = 1  # Mono audio\n",
    "RATE = 16000  # sampling rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d9ac16b-29bb-4788-a8f6-c892dcef918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_16k_mono_from_buffer(audio_bytes, dtype=np.int16):\n",
    "    \"\"\"\n",
    "    Converts raw audio bytes from PyAudio buffer to float32 tensor\n",
    "    equivalent to tf.audio.decode_wav(..., desired_channels=1)\n",
    "    \"\"\"\n",
    "    # Convert bytes to int16 NumPy array\n",
    "    audio_np = np.frombuffer(audio_bytes, dtype=dtype)\n",
    "\n",
    "    # Normalize to float32 in [-1.0, 1.0]\n",
    "    audio_float32 = audio_np.astype(np.float32) / 32768.0\n",
    "\n",
    "    # Convert to TensorFlow tensor\n",
    "    audio_tensor = tf.convert_to_tensor(audio_float32, dtype=tf.float32)\n",
    "\n",
    "    return audio_tensor  # shape: [samples], dtype: float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ceb57aa-7142-48ac-9cc3-b480628cd8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sethwright/Documents/audio-model/output/heychef_float32.tflite\n"
     ]
    }
   ],
   "source": [
    "model_path=\"/Users/sethwright/Documents/audio-model/output/heychef_float32.tflite\"\n",
    "print(model_path)\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_SECONDS = 1.25             # ≈1.25 s\n",
    "NUM_SAMPLES = int(SAMPLE_RATE * DURATION_SECONDS)  # 20000\n",
    "NUM_MELS = 40\n",
    "FRAME_LENGTH = 512\n",
    "FRAME_STEP = 160                    # 10 ms step\n",
    "FFT_LENGTH = 512\n",
    "FMIN = 80.0\n",
    "FMAX = 7600.0\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 30\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e6c5e6e-7fb1-4fe6-8f32-1fcadacff2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_from_buffer(audio_tensor):\n",
    "    \"\"\"\n",
    "    Convert a raw audio buffer to a normalized log-mel spectrogram\n",
    "    compatible with the training pipeline.\n",
    "    \"\"\"\n",
    "    # Ensure correct length (pad or trim to NUM_SAMPLES = 20000)\n",
    "    audio_tensor = audio_tensor[:NUM_SAMPLES]\n",
    "    pad_len = NUM_SAMPLES - tf.shape(audio_tensor)[0]\n",
    "    audio_tensor = tf.pad(audio_tensor, [[0, pad_len]])\n",
    "\n",
    "    # Pre-emphasis filter to boost high frequencies\n",
    "    emphasized = tf.concat([audio_tensor[:1], audio_tensor[1:] - 0.97 * audio_tensor[:-1]], axis=0)\n",
    "\n",
    "    # Compute STFT\n",
    "    stft = tf.signal.stft(\n",
    "        emphasized,\n",
    "        frame_length=FRAME_LENGTH,\n",
    "        frame_step=FRAME_STEP,\n",
    "        fft_length=FFT_LENGTH,\n",
    "        window_fn=tf.signal.hann_window\n",
    "    )\n",
    "    mag = tf.abs(stft)\n",
    "\n",
    "    # Apply mel filterbank\n",
    "    mel_weight = tf.signal.linear_to_mel_weight_matrix(\n",
    "        NUM_MELS,\n",
    "        mag.shape[-1],\n",
    "        SAMPLE_RATE,\n",
    "        FMIN,\n",
    "        FMAX\n",
    "    )\n",
    "    mel = tf.matmul(mag, mel_weight)\n",
    "\n",
    "    # Log scale and normalization\n",
    "    log_mel = tf.math.log(mel + 1e-6)\n",
    "    mean = tf.reduce_mean(log_mel)\n",
    "    std = tf.math.reduce_std(log_mel) + 1e-6\n",
    "    log_mel = (log_mel - mean) / std\n",
    "\n",
    "    # Add channel dimension\n",
    "    log_mel = tf.expand_dims(log_mel, axis=-1)\n",
    "\n",
    "    return log_mel  # shape: [time_frames, NUM_MELS, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d58ad-d603-4f1c-9d50-076ae0ba1058",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_from_buffer_old(audio_tensor):\n",
    "    target_len = 20000\n",
    "    audio_tensor = audio_tensor[:target_len]\n",
    "    zero_padding = tf.zeros([target_len] - tf.shape(audio_tensor), dtype=tf.float32)\n",
    "    audio_tensor = tf.concat([audio_tensor, zero_padding], axis=0)\n",
    "\n",
    "    spectrogram = tf.signal.stft(audio_tensor, frame_length=512, frame_step=160)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "\n",
    "    return spectrogram  # shape: [?]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75a40d-7ab2-4168-91d0-b4518a5b42a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path)\n",
    "interpreter.allocate_tensors()\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Print input/output details\n",
    "print(\"Input details:\", input_details)\n",
    "print(\"Output details:\", output_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058679c-65e4-4661-a005-95eb72bb9e74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available input devices:\n",
      "  [0] MacBook Pro Microphone\n"
     ]
    }
   ],
   "source": [
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio() \n",
    "audio_buffer = deque(maxlen=20000)\n",
    "\n",
    "# List input devices\n",
    "print(\"Available input devices:\")\n",
    "for i in range(audio.get_device_count()):\n",
    "    info = audio.get_device_info_by_index(i)\n",
    "    if info[\"maxInputChannels\"] > 0:\n",
    "        print(f\"  [{i}] {info['name']}\")\n",
    "# Open stream\n",
    "stream = audio.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "while True:\n",
    "    data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "    samples = np.frombuffer(data, dtype=np.int16).astype(np.float32)\n",
    "    audio_buffer.extend(samples)\n",
    "\n",
    "    if len(audio_buffer) == 20000:\n",
    "        wav_tensor = np.array(audio_buffer, dtype=np.float32)\n",
    "        spectrogram = preprocess_from_buffer(wav_tensor)\n",
    "        spectrogram = tf.expand_dims(spectrogram, axis=0)\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], spectrogram)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])[0][0]\n",
    "\n",
    "        if output > 0.5:\n",
    "            print(\"Wake word detected!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984dc98-bc62-4670-b9d7-fa2ad1ce2962",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "# audio_buffer = deque(28000)\n",
    "\n",
    "# List input devices\n",
    "print(\"Available input devices:\")\n",
    "for i in range(audio.get_device_count()):\n",
    "    info = audio.get_device_info_by_index(i)\n",
    "    if info[\"maxInputChannels\"] > 0:\n",
    "        print(f\"  [{i}] {info['name']}\")\n",
    "# Open stream\n",
    "stream = audio.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"\\nRecording...\")\n",
    "try:\n",
    "    while True:\n",
    "        audio_buffer = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        # Now process it\n",
    "        wav_tensor = load_wav_16k_mono_from_buffer(audio_buffer)\n",
    "        spectrogram = preprocess_from_buffer(wav_tensor)\n",
    "        spectrogram = tf.expand_dims(spectrogram, axis=0)\n",
    "\n",
    "        print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "        interpreter.set_tensor(input_details[0]['index'], spectrogram)\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "        # Get the output\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output_data_number = [1 if output_data > .5 else 0] \n",
    "        print(\"Output:\", output_data, output_data_number)\n",
    "        # You can now run inference here...\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by user.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7032f35-9191-4651-9fa0-0656d1da8828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded: /Users/sethwright/Documents/audio-model/output/saved_model.keras\n"
     ]
    }
   ],
   "source": [
    "# Load trained Keras model\n",
    "# -----------------------\n",
    "MODEL_PATH=\"/Users/sethwright/Documents/audio-model/output/saved_model.keras\"\n",
    "model = tf.keras.models.load_model(MODEL_PATH, compile =False, safe_mode=False)\n",
    "print(\"✅ Model loaded:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2238154-24f7-40d4-84f5-8460a0190a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run real time predictions with keras model\n",
    "def test_audio():\n",
    "    # Initialize PyAudio\n",
    "    audio = pyaudio.PyAudio() \n",
    "    audio_buffer = deque(maxlen=20000)\n",
    "\n",
    "    # List input devices\n",
    "    #print(\"Available input devices:\")\n",
    "    for i in range(audio.get_device_count()):\n",
    "        info = audio.get_device_info_by_index(i)\n",
    "        #if info[\"maxInputChannels\"] > 0:\n",
    "        #print(f\"  [{i}] {info['name']}\")\n",
    "    # Open stream\n",
    "    stream = audio.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "    print(\"listening started\")\n",
    "    while True:\n",
    "        data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        samples = np.frombuffer(data, dtype=np.int16).astype(np.float32)\n",
    "        audio_buffer.extend(samples)\n",
    "\n",
    "        if len(audio_buffer) == 20000:\n",
    "            wav_tensor = np.array(audio_buffer, dtype=np.float32)\n",
    "            spectrogram = preprocess_from_buffer(wav_tensor)\n",
    "            spectrogram = tf.expand_dims(spectrogram, axis=0)\n",
    "\n",
    "            pred = model.predict(spectrogram,verbose=0)[0][0]\n",
    "       \n",
    "\n",
    "\n",
    "            if pred > 0.9:\n",
    "                print(\"Wake word detected!\")\n",
    "                print(f\"\\nModel raw output: {pred:.4f}\")\n",
    "                playsound(\"/Users/sethwright/Downloads/gong.mp3\")\n",
    "                stream.stop_stream()\n",
    "                stream.close()\n",
    "                audio.terminate()\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2d2188a-cdd6-4709-94c1-165628e0e7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listening started\n",
      "Wake word detected!\n",
      "\n",
      "Model raw output: 0.9104\n"
     ]
    }
   ],
   "source": [
    "test_audio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-kernel",
   "language": "python",
   "name": "audio-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
